---
# You don't need to edit this file, it's empty on purpose.
# Edit theme's home layout instead if you wanna make some changes
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
layout: home
author_profile: true
---

# Data Engineer

Backend engineer with a focus on building efficient, scalable server-side applications in Python. Experienced in designing APIs, implementing effective algorithms, and integrating third-party services. Known for quickly adapting to new technologies and leveraging them to create robust, high-performing solutions.

## About Me

Hi, I'm Kosala, a Data Engineer specializing in:
- Data pipeline development
- ETL/ELT processes
- Cloud data platforms (AWS, GCP, Azure)
- Big Data technologies (Spark, Hadoop)
- SQL & NoSQL databases (PostgreSQL, MySQL, MSSQL, Neo4j)

## Skills

- Programming: Python, Java, JavaScript, SQL, Bash
- Data: Spark, Hadoop, Airflow, Kafka, Snowflake
- Cloud: AWS (EC2, S3, SQS, ECR, EKS, Lambda, CF (CloudFormation)), Azure (Blob storage, Service bus, Key vault, AKS)
- Version Control: Git, GitHub
- Containerization: Docker, Kubernetes

---

# Work Experience

## Data Engineer BlackSwan Technologies | Feb 2023 - Present

### Projects:

**1. Data Fabric and Knowledge Graph for fin-tech customer i.e. ING and Deutscher bank**

- Worked on developing Restful microservices using Python flask which initiates data ingestion jobs in the knowledge
mesh.
- Worked on enhancing source data invocation services which extract data using GraphQL based wrappers.
- Redesigned and implemented Job tracking service for a distributed micro services.
- Created a configurable test suite to evaluate performance and integrity of a queue-based data processing application.
- Created a library which process queue messages asynchronously.
- Followed the Git Flow branching model for source code management.

**2. Web Scrapers**

- Developed online data extraction applications from source websites(Used html parser libraries and selenium for interactive web scraping).
- Integrated web scraper data using a pipeline(set of microservices) to a domain specific schema.
- Deployed applications on AWS CloudFormation stacks.
- Developed Java-based SOAP client to extract data, transform it into a JSON structure and map it to a common domain-compliant schema.

**3. Data Screening**

- Built an application(microservice) that processes files on a file server.
- The processing involves parsing XML files in batches, transforming the data into a domain-specific format, and uploading the transformed data to an Azure Blob container for the downstream consumption.

## Intern - Data Engineer Wiley | Jan 2022 - Jan 2023

# Projects

## 1. Real-Time Data Pipeline
- Built a streaming pipeline using Kafka and Spark Streaming.
- Deployed on AWS with automated scaling.

## 2. ETL for E-commerce Analytics
- Designed ETL workflows in Airflow.
- Integrated multiple data sources into a Redshift data warehouse.

## 3. Data Lake Implementation
- Architected a data lake on AWS S3.
- Automated ingestion and transformation using Glue.

## Contact

- Email: [your.email@example.com]
- LinkedIn: [Your LinkedIn Profile]
- GitHub: [Your GitHub Profile]
# kosalaMaduwantha.github.io
portfolio site 
