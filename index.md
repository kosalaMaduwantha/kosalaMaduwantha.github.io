---
# You don't need to edit this file, it's empty on purpose.
# Edit theme's home layout instead if you wanna make some changes
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
title: "Profile"
layout: single
author_profile: true
# sidebar:
#   nav: "home"
toc: true
toc_sticky: true
---

## About

Data Engineer, having 3 plus years of industrial experience specialized in designing and implementing scalable server-side applications and data ingestion pipelines. Proficient in Python, with experience in API development, third-party service integration, and building scalable data pipelines from various data sources. Adept at learning new technologies and applying them effectively to create high-performance, maintainable solutions.

## Interests

- Data pipeline development
- ETL/ELT processes
- AI/ML applications development (RAG, Agent-based systems)
- Cloud data platforms 
- Big Data technologies 
- SQL & NoSQL databases 

## Skills

- Programming: Python, Java, JavaScript, SQL, Bash
- Data: Spark, Hadoop, Airflow, Kafka, Snowflake
- Cloud: AWS (EC2, S3, SQS, ECR, EKS, Lambda, CF (CloudFormation)), Azure (Blob storage, Service bus, Key vault, AKS)
- Version Control: Git, GitHub
- Containerization: Docker, Kubernetes


## Work Experience

### Data Engineer BlackSwan Technologies | Feb 2023 - Present

#### Projects:

**1. Data Fabric and Knowledge Graph for fin-tech customer i.e. ING and Deutscher bank**

- Worked on developing Restful microservices using Python flask which initiates data ingestion jobs in the knowledge
mesh.
- Worked on enhancing source data invocation services which extract data using GraphQL based wrappers.
- Redesigned and implemented Job tracking service for a distributed micro services.
- Created a configurable test suite to evaluate performance and integrity of a queue-based data processing application.
- Created a library which process queue messages asynchronously.
- Followed the Git Flow branching model for source code management.

**2. Web Scrapers**

- Developed online data extraction applications from source websites(Used html parser libraries and selenium for interactive web scraping).
- Integrated web scraper data using a pipeline(set of microservices) to a domain specific schema.
- Deployed applications on AWS CloudFormation stacks.
- Developed Java-based SOAP client to extract data, transform it into a JSON structure and map it to a common domain-compliant schema.

**3. Data Screening**

- Built an application(microservice) that processes files on a file server.
- The processing involves parsing XML files in batches, transforming the data into a domain-specific format, and uploading the transformed data to an Azure Blob container for the downstream consumption.

### Intern - Data Engineer Wiley | Jan 2022 - Jan 2023

- Set up metadata-driven data pipelines to bring data from various data sources.
- Worked with multiple data sources, bring the data together into a data lake for easier access and analysis.
- Monitored data pipelines, fixing issues right away to prevent disruptions.
- Moved data workflows from Tidal to Apache Airflow, making the automation process simpler and faster.
- Leveraged a range of technologies including Python, Bash, AWS S3, Apache Airflow, Snowflake Cloud Datawarehouse, MySQL and MSSQL to achieve project objectives.

## Personal Projects

<a href="/personal-projects/" class="btn btn--primary">View Personal Projects</a>

## Contact

- Email: [kosalamaduwantha0@gmail.com](mailto:kosalamaduwantha0@gmail.com)
- LinkedIn: [LinkedIn Profile](https://www.linkedin.com/in/kosala-maduwantha/)
- GitHub: [GitHub Profile](https://github.com/kosalaMaduwantha)
# kosalaMaduwantha.github.io
portfolio site 
